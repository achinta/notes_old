{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGznawH6xTBn"
   },
   "source": [
    "# Paper - AR-Net\n",
    "> A simple Auto-Regressive Neural Network for time series\n",
    "\n",
    "- layout: post\n",
    "- categories: [paper]\n",
    "- search_exclude: true\n",
    "\n",
    "https://arxiv.org/abs/1911.12436\n",
    "\n",
    "## Introduction\n",
    "#### What is auto-regression? \n",
    "An autoregressive model is when a value from a time series is regressed on previous values from that same time series. for example,\n",
    "$$\n",
    "y_t = \\beta_0 + {\\beta_1}{y_{t-1}} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "The order of an autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time. So, the preceding model is a first-order autoregression, written as AR(1).\n",
    "\n",
    "## Classic-AR model vs AR-Net\n",
    "<blockquote>\n",
    "We formulate a simple neural network that mimics the Classic-AR model, with the only difference being how they are\n",
    "fitted to data. Our model termed AR-Net, in itâ€™s simplest form, is identical to linear regression, fitted with stochastic\n",
    "gradient descent (SGD). We show that AR-Net is identically interpretable as a Classic-AR model and scales to large\n",
    "p-orders. As we discuss in the future work section, our vision is to leverage more powerful temporal modeling\n",
    "techniques of deep learning without sacrificing interpretability via explicit modeling of time-series components\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "We intentionally did not use more powerful methods, such as modeling latent states with recurrent networks or convolution because our goal\n",
    "was to bridge, not widen, the gap between traditional time-series and deep learning methods. We hope to show with\n",
    "AR-Net that deep learning models can be simple, interpretable, fast and easy to use, so that the time-series community\n",
    "may consider deep learning a viable option.\n",
    "</blockquote>\n",
    "\n",
    "|   | Statistical Models                                                                  | Neural Networks                                                                                                                           |\n",
    "|---|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|   | Concise model, but we need to make strong assumptions on data such as the p-order.  | Non-parametric data driven model and it does not require restrictive assumptions on the underlying process from which data are generated. |\n",
    "|   | Models with large p-order become slow                                               |                                                                                                                                           |\n",
    "|   |                                                                                     | Have non-linear function mapping capability which can approximate any continuous function, hence can solve many complex problems          |\n",
    "\n",
    "## Contribution\n",
    "\n",
    "\n",
    "## Methods\n",
    "### Classic AR\n",
    "AR model or order *p* can be written as \n",
    "$$\n",
    "y_t = c + \\sum_{i=1}^{i=p}w_i*y_{t-i} + e_t\n",
    "$$\n",
    "\n",
    "$e_t$ is the noise.\n",
    "\n",
    "### AR Net model\n",
    "AR Net model is a neural network whose parameters in the first layer are equivalent to the classic AR coefficients. *AR Net* can be extended with hidden layers to achieve greater forecasting accuracy, **at the cost of direct interpretability**. Loss used is MSE, to keep it comparable with *classic AR*. \n",
    "$$\n",
    "L(y, \\hat{y}, \\theta) = \\frac{1}{n}\\sum_1^n(y-\\hat{y}_\\theta)^2\n",
    "$$\n",
    "\n",
    "### Sparse AR Net\n",
    "<blockquote>\n",
    "In order to relax the constraint of knowing the true AR order, we can fit a larger model with sparse AR coefficients.\n",
    "This will also do away with the assumption that the AR-coefficients must consist of consecutive lags. We achieve this\n",
    "by adding a regularization term R to the loss L being minimized.\n",
    "</blockquote>\n",
    "TODO\n",
    "\n",
    "#### References\n",
    "https://online.stat.psu.edu/stat501/lesson/14/14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFtdcqEjyWFT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_xRw7nhyLdE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
