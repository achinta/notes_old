{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers from scratch in pytorch\n",
    "> Notes from https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
    "\n",
    "- layout: post\n",
    "- toc: false\n",
    "- comments: false\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- categories: [fastpages, jupyter]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "## Standard Libraries\n",
    "import os\n",
    "from tracemalloc import Snapshot\n",
    "import numpy as numpy\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats()\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# others\n",
    "import einops\n",
    "\n",
    "# path to folder where datasets should be downloaded\n",
    "DATASET_PATH = \"../data\"\n",
    "CHECKPOINT_PATH = \"../saved_models\"\n",
    "\n",
    "# setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministirc on GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device: \", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention?\n",
    "*The attention mechanism describes a weighted average of sequence of elements with the weights dynamically computed based on the input query and the element's keys.*\n",
    "\n",
    "\n",
    "![image](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
    "As we can see above, Query, Key and Value are all just modified word embedding. \n",
    "\n",
    "![image](http://jalammar.github.io/images/t/self-attention_softmax.png)\n",
    "\n",
    "\n",
    "(image from http://jalammar.github.io/illustrated-transformer/)\n",
    "#### Self-attention\n",
    "We use self-attention to modify each embedding of the input word as a combination of modified `values` from the word embedding and the weights computed. Some intuitions\n",
    "\n",
    "\n",
    " - `Values` are nothing but modified word embeddings which will be weighted and given as outputs\n",
    " - `Query` is the modified word embedding which will search for relevant word embeddings in the sequence\n",
    " - `Keys` are modified word embeddings which will be searched by the query to create the weights\n",
    " - As we are using dot product, we are essentially computing the cosine similarity between the query and the key. The higher the cosine similarity, the higher the weight.\n",
    "\n",
    "If we look at the softmax scores, clearly the word at its own position will have the highest softmax score, but sometimes itâ€™s useful to attend to another word that is relevant to the current word.\n",
    "\n",
    "Lets look at the statement `The animal didn't cross the street because it was too tired`.  In this case, softmax for 'it' will have a parts of itself, `animal` (more) and `street` (less) \n",
    "\n",
    "In addition to `attending` to other parts of sentence, the softmax can also drown out the irrelvant words by multiplying them with a very small number.\n",
    "\n",
    "![image](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "\n",
    "### Multi Head Attention\n",
    "An attention layer outputs a representation of the input sequence, based on the weights learnt for Query, Key and Value. We use multiple heads and combine them using a linear layer to get a better representation of the input sequence.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits/math.sqrt(d_k) \n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask==0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n",
      "Attention\n",
      " torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.size())\n",
    "print(\"K\\n\", k.size())\n",
    "print(\"V\\n\", v.size())\n",
    "print(\"Values\\n\", values.size())\n",
    "print(\"Attention\\n\", attention.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:MultiHeadAttention:x.shape - torch.Size([3, 10, 64]). qkv_proj.shape - torch.Size([48, 64])\n",
      "DEBUG:MultiHeadAttention:generated qkv.shape - torch.Size([3, 10, 48])\n",
      "DEBUG:MultiHeadAttention:reshaped qkv.shape - torch.Size([3, 10, 4, 12])\n",
      "DEBUG:MultiHeadAttention:permuted qkv.shape - torch.Size([3, 4, 10, 12])\n",
      "DEBUG:MultiHeadAttention:values.shape - torch.Size([3, 4, 10, 4]). attension.shape - torch.Size([3, 4, 10, 10])\n",
      "DEBUG:MultiHeadAttention:values.shape after permute - torch.Size([3, 10, 4, 4])\n",
      "DEBUG:MultiHeadAttention:values.shape after reshape - torch.Size([3, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, log_level=logging.INFO):\n",
    "        super().__init__()\n",
    "        assert embed_dim%num_heads == 0, \"Embedding dimension should be zero module with number of heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim//num_heads\n",
    "        # logging.getLogger().setLevel(logging.DEBUG)\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        # print(f'log_level - {log_level}')\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # stack all weight matrices 1...h together for efficiency\n",
    "        # Note that in many implemenations, you see bias=false, which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim) \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Orignial Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        self.logger.debug(f'x.shape - {x.size()}. qkv_proj.shape - {self.qkv_proj.weight.size()}')\n",
    "\n",
    "        # separate Q, K, V from the linear output\n",
    "        self.logger.debug(f'generated qkv.shape - {qkv.size()}')\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        self.logger.debug(f'reshaped qkv.shape - {qkv.size()}')\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, seqlen, Head, DIms ]\n",
    "        self.logger.debug(f'permuted qkv.shape - {qkv.size()}')\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        self.logger.debug(f'values.shape - {values.size()}. attension.shape - {attention.size()}')\n",
    "        values = values.permute(0, 2, 1, 3) # [Batch, seqlen, Head, dims]\n",
    "        self.logger.debug(f'values.shape after permute - {values.size()}')\n",
    "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        self.logger.debug(f'values.shape after reshape - {values.size()}')\n",
    "\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "input_dim = 64\n",
    "embed_dim = 16\n",
    "batch_size = 3\n",
    "seq_len = 10\n",
    "multi_attention = MultiHeadAttention(input_dim, embed_dim, num_heads=4, log_level=logging.DEBUG)\n",
    "x = torch.randn(batch_size, seq_len, input_dim)\n",
    "att = multi_attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # attention layer\n",
    "        self.self_attn = MultiHeadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # two layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "\n",
    "        # layers to apply inbetween main layers\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention part\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # MLP part\n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for l in self.layers:\n",
    "            x = l(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for l in self.layers:\n",
    "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = l(x)\n",
    "        return attention_maps\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        # create a matrix of seq_len, hidden_dim representing the positinal encoding for \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # position is index of the word in the sequence\n",
    "        position = torch.arange(0,  max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)) * -math.log(10000)/d_model\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # register_buffer => tensor which is not a parameter, but shold be a part of the modules state\n",
    "        # used for tensors that need to be on the same device as the module\n",
    "        # persistent=False tell pytorch not to add the buffer to the state_dict\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(n) =\n",
    "\\begin{cases}\n",
    "n/2,  & \\text{if $n$ is even} \\\\\n",
    "3n+1, & \\text{if $n$ is odd}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "We add a fixed signal (not trainable) to each word based on its position. The dimension of the PE signal is same is same as the word dimension. \n",
    "\n",
    "$$\n",
    "PE_{(pos, i)} = \n",
    "\\begin{cases}\n",
    "    sin(\\frac{pos}{10000^{{i}/d_{model}}}),  & \\text{if $i$ mod 2 = 0}\\\\\n",
    "    cos(\\frac{pos}{10000^{{i-1}/d_{model}}}),  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here pos is the word position and i is the embedding position. \n",
    "\n",
    "For a deeper intuition, look at \n",
    " - https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    " - https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:25:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e254e10ce936db31950534e6283aea55ae2f493e2c9033656d0427d3a6e205d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
