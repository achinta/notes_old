{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGznawH6xTBn"
      },
      "source": [
        "# Paper: AR-Net\n",
        "> A simple Aut-Regressive Neural Network for time series\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "<!-- - badges: true -->\n",
        "<!-- - comments: true -->\n",
        "- categories: [paper]\n",
        "<!-- - image: images/some_folder/your_image.png -->\n",
        "- hide: false\n",
        "- search_exclude: true\n",
        "<!-- - metadata_key1: metadata_value1 -->\n",
        "<!-- - metadata_key2: metadata_value2 -->\n",
        "\n",
        "https://arxiv.org/abs/1911.12436\n",
        "\n",
        "## Introduction\n",
        "#### What is auto-regression? \n",
        "An autoregressive model is when a value from a time series is regressed on previous values from that same time series. for example,\n",
        "$$\n",
        "y_t = \\beta_0 + {\\beta_1}{y_{t-1}} + \\epsilon_t\n",
        "$$\n",
        "\n",
        "The order of an autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time. So, the preceding model is a first-order autoregression, written as AR(1).\n",
        "\n",
        "### Contribution\n",
        "<blockquote>\n",
        "We formulate a simple neural network that mimics the Classic-AR model, with the only difference being how they are\n",
        "fitted to data. Our model termed AR-Net, in itâ€™s simplest form, is identical to linear regression, fitted with stochastic\n",
        "gradient descent (SGD). We show that AR-Net is identically interpretable as a Classic-AR model and scales to large\n",
        "p-orders. As we discuss in the future work section, our vision is to leverage more powerful temporal modeling\n",
        "techniques of deep learning without sacrificing interpretability via explicit modeling of time-series components\n",
        "\n",
        "We intentionally did not use more powerful methods, such as modeling latent states with recurrent networks or convolution because our goal\n",
        "was to bridge, not widen, the gap between traditional time-series and deep learning methods. We hope to show with\n",
        "AR-Net that deep learning models can be simple, interpretable, fast and easy to use, so that the time-series community\n",
        "may consider deep learning a viable option.\n",
        "</blockquote>\n",
        "\n",
        "#### References\n",
        "https://online.stat.psu.edu/stat501/lesson/14/14.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtdcqEjyWFT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_xRw7nhyLdE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}